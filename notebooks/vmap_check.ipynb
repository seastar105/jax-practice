{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.nnx as nnx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.model import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rngs = nnx.Rngs(0)\n",
    "layer = nnx.Linear(768, 768 * 2, rngs=rngs, use_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jax.random.normal(jax.random.PRNGKey(0), (4, 768))\n",
    "temp = layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, gate = jnp.split(temp, 2, axis=-1)\n",
    "x.shape, gate.shape\n",
    "\n",
    "jnp.allclose(temp, jnp.concat([x, gate], axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ids = jnp.array([0, 0, 0, 1, 1, 2, 2, 2, 2, 2]).reshape(1, -1)\n",
    "causal_mask = nnx.make_causal_mask(document_ids)\n",
    "print(causal_mask)\n",
    "doc_mask = nnx.make_attention_mask(document_ids, document_ids, jnp.equal)\n",
    "print(doc_mask)\n",
    "final_mask = nnx.combine_masks(causal_mask, doc_mask)\n",
    "print(final_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, x):\n",
    "    logits = model(x)\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits.reshape(-1, 10000), x.reshape(-1))\n",
    "    return loss\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(model, optimizer, x):\n",
    "    def final_loss_fn(model, x):\n",
    "        loss = loss_fn(model, x)\n",
    "        return loss.mean()\n",
    "\n",
    "    grad_fn = nnx.value_and_grad(final_loss_fn)\n",
    "    loss, grads = grad_fn(model, x)\n",
    "    optimizer.update(grads)\n",
    "    return loss\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def vmap_train_step(model, optimizer, x):\n",
    "    vmap_loss_fn = nnx.vmap(loss_fn, in_axes=(None, 0))\n",
    "\n",
    "    def final_loss_fn(model, x):\n",
    "        loss = vmap_loss_fn(model, x)\n",
    "        return loss.mean()\n",
    "\n",
    "    grad_fn = nnx.value_and_grad(final_loss_fn)\n",
    "    loss, grads = grad_fn(model, x)\n",
    "    optimizer.update(grads)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    vocab_size=10000,\n",
    "    num_layers=4,\n",
    "    dim=512,\n",
    "    dim_ff=2048,\n",
    "    num_heads=8,\n",
    "    rngs=nnx.Rngs(params=0, dropout=0),\n",
    "    context_length=1024,\n",
    "    ff_activation=\"gelu\",\n",
    "    ff_dropout=0.0,\n",
    "    attention_dropout=0.0,\n",
    "    residual_dropout=0.0,\n",
    "    use_bias=False,\n",
    "    norm_class=\"rmsnorm\",\n",
    "    use_glu=True,\n",
    ")\n",
    "optimizer = nnx.Optimizer(model, optax.adam(learning_rate=1e-4))\n",
    "\n",
    "losses = []\n",
    "batch_size = 16\n",
    "context_length = 1024\n",
    "\n",
    "train_step(model, optimizer, jax.random.randint(jax.random.PRNGKey(0), (batch_size, context_length), 0, 10000))\n",
    "for _ in tqdm(range(50)):\n",
    "    losses.append(\n",
    "        train_step(model, optimizer, jax.random.randint(jax.random.PRNGKey(0), (batch_size, context_length), 0, 10000))\n",
    "    )\n",
    "print(losses[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import orbax.checkpoint as ocp\n",
    "\n",
    "\n",
    "def save_state(state, out: str):\n",
    "    checkpointer = ocp.StandardCheckpointer()\n",
    "    # nnx using jax.random.key, but it seems orbax not supporting now. Workaround is do not save rng_state\n",
    "    graphdef, rng_state, other_state = nnx.split(state, nnx.RngState, ...)\n",
    "    checkpointer.save(out, other_state)\n",
    "    checkpointer.wait_until_finished()\n",
    "\n",
    "\n",
    "def load_state(src: str, state):\n",
    "    checkpointer = ocp.StandardCheckpointer()\n",
    "    graphdef, rng_state, other_state = nnx.split(state, nnx.RngState, ...)\n",
    "    state_restored = checkpointer.restore(src, other_state)\n",
    "    nnx.update(state, state_restored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree(\"/home/seastar105/jax-practice/checkpoints/save_test\", ignore_errors=True)\n",
    "save_state(optimizer, \"/home/seastar105/jax-practice/checkpoints/save_test\")\n",
    "\n",
    "restored_state = nnx.Optimizer(\n",
    "    Transformer(\n",
    "        vocab_size=10000,\n",
    "        num_layers=4,\n",
    "        dim=512,\n",
    "        dim_ff=2048,\n",
    "        num_heads=8,\n",
    "        rngs=nnx.Rngs(params=0, dropout=0),\n",
    "        context_length=1024,\n",
    "        ff_activation=\"gelu\",\n",
    "        ff_dropout=0.0,\n",
    "        attention_dropout=0.0,\n",
    "        residual_dropout=0.0,\n",
    "        use_bias=False,\n",
    "        norm_class=\"rmsnorm\",\n",
    "        use_glu=True,\n",
    "    ),\n",
    "    optax.adam(learning_rate=1e-4),\n",
    ")\n",
    "\n",
    "load_state(\"/home/seastar105/jax-practice/checkpoints/save_test\", restored_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = nnx.state(optimizer.model)\n",
    "restored = nnx.state(restored_state.model)\n",
    "\n",
    "\n",
    "def compare_arrays(x, y):\n",
    "    if isinstance(x, jnp.ndarray) and isinstance(y, jnp.ndarray):\n",
    "        return jnp.array_equal(x, y)\n",
    "    elif isinstance(x, jax.Array) and isinstance(y, jax.Array):\n",
    "        return jnp.array_equal(x, y)\n",
    "    else:\n",
    "        return x == y\n",
    "\n",
    "\n",
    "# Compare the pytrees\n",
    "comparison_result = jax.tree_util.tree_map(compare_arrays, restored, orig)\n",
    "# Check if all leaves are True\n",
    "all_equal = all(jax.tree_util.tree_leaves(comparison_result))\n",
    "print(f\"Are the states equal? {all_equal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    vocab_size=10000,\n",
    "    num_layers=4,\n",
    "    dim=768,\n",
    "    dim_ff=2048,\n",
    "    num_heads=8,\n",
    "    rngs=nnx.Rngs(params=0, dropout=0),\n",
    "    context_length=1024,\n",
    "    ff_activation=\"gelu\",\n",
    "    ff_dropout=0.0,\n",
    "    attention_dropout=0.0,\n",
    "    residual_dropout=0.0,\n",
    "    use_bias=False,\n",
    "    norm_class=\"rmsnorm\",\n",
    "    use_glu=True,\n",
    ")\n",
    "optimizer = nnx.Optimizer(model, optax.adam(learning_rate=1e-4))  # nnx.Optimizer is used like flax.training.TrainState\n",
    "\n",
    "losses = []\n",
    "vmap_train_step(model, optimizer, jax.random.randint(jax.random.PRNGKey(0), (batch_size, context_length), 0, 10000))\n",
    "for _ in tqdm(range(500)):\n",
    "    losses.append(\n",
    "        vmap_train_step(\n",
    "            model, optimizer, jax.random.randint(jax.random.PRNGKey(0), (batch_size, context_length), 0, 10000)\n",
    "        )\n",
    "    )\n",
    "print(losses[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
