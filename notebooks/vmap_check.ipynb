{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.nnx as nnx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.model import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rngs = nnx.Rngs(0)\n",
    "layer = nnx.Linear(768, 768 * 2, rngs=rngs, use_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jax.random.normal(jax.random.PRNGKey(0), (4, 768))\n",
    "temp = layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, gate = jnp.split(temp, 2, axis=-1)\n",
    "x.shape, gate.shape\n",
    "\n",
    "jnp.allclose(temp, jnp.concat([x, gate], axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ids = jnp.array([0, 0, 0, 1, 1, 2, 2, 2, 2, 2]).reshape(1, -1)\n",
    "causal_mask = nnx.make_causal_mask(document_ids)\n",
    "print(causal_mask)\n",
    "doc_mask = nnx.make_attention_mask(document_ids, document_ids, jnp.equal)\n",
    "print(doc_mask)\n",
    "final_mask = nnx.combine_masks(causal_mask, doc_mask)\n",
    "print(final_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, x):\n",
    "    logits = model(x)\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits.reshape(-1, 10000), x.reshape(-1))\n",
    "    return loss\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(model, optimizer, x):\n",
    "    def final_loss_fn(model, x):\n",
    "        loss = loss_fn(model, x)\n",
    "        return loss.mean()\n",
    "\n",
    "    grad_fn = nnx.value_and_grad(final_loss_fn)\n",
    "    loss, grads = grad_fn(model, x)\n",
    "    optimizer.update(grads)\n",
    "    return loss\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def vmap_train_step(model, optimizer, x):\n",
    "    vmap_loss_fn = nnx.vmap(loss_fn, in_axes=(None, 0))\n",
    "\n",
    "    def final_loss_fn(model, x):\n",
    "        loss = vmap_loss_fn(model, x)\n",
    "        return loss.mean()\n",
    "\n",
    "    grad_fn = nnx.value_and_grad(final_loss_fn)\n",
    "    loss, grads = grad_fn(model, x)\n",
    "    optimizer.update(grads)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    vocab_size=10000,\n",
    "    num_layers=4,\n",
    "    dim=512,\n",
    "    dim_ff=2048,\n",
    "    num_heads=8,\n",
    "    rngs=nnx.Rngs(params=0, dropout=0),\n",
    "    context_length=1024,\n",
    "    ff_activation=\"gelu\",\n",
    "    ff_dropout=0.0,\n",
    "    attention_dropout=0.0,\n",
    "    residual_dropout=0.0,\n",
    "    use_bias=False,\n",
    "    norm_class=\"rmsnorm\",\n",
    "    use_glu=True,\n",
    ")\n",
    "optimizer = nnx.Optimizer(model, optax.adam(learning_rate=1e-4))\n",
    "\n",
    "losses = []\n",
    "batch_size = 64\n",
    "context_length = 1024\n",
    "\n",
    "train_step(model, optimizer, jax.random.randint(jax.random.PRNGKey(0), (batch_size, context_length), 0, 10000))\n",
    "for _ in tqdm(range(500)):\n",
    "    losses.append(\n",
    "        train_step(model, optimizer, jax.random.randint(jax.random.PRNGKey(0), (batch_size, context_length), 0, 10000))\n",
    "    )\n",
    "print(losses[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    vocab_size=10000,\n",
    "    num_layers=4,\n",
    "    dim=768,\n",
    "    dim_ff=2048,\n",
    "    num_heads=8,\n",
    "    rngs=nnx.Rngs(params=0, dropout=0),\n",
    "    context_length=1024,\n",
    "    ff_activation=\"gelu\",\n",
    "    ff_dropout=0.0,\n",
    "    attention_dropout=0.0,\n",
    "    residual_dropout=0.0,\n",
    "    use_bias=False,\n",
    "    norm_class=\"rmsnorm\",\n",
    "    use_glu=True,\n",
    ")\n",
    "optimizer = nnx.Optimizer(model, optax.adam(learning_rate=1e-4))\n",
    "\n",
    "losses = []\n",
    "vmap_train_step(model, optimizer, jax.random.randint(jax.random.PRNGKey(0), (batch_size, context_length), 0, 10000))\n",
    "for _ in tqdm(range(500)):\n",
    "    losses.append(\n",
    "        vmap_train_step(\n",
    "            model, optimizer, jax.random.randint(jax.random.PRNGKey(0), (batch_size, context_length), 0, 10000)\n",
    "        )\n",
    "    )\n",
    "print(losses[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
