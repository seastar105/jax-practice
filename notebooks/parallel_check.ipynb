{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "import jax.experimental\n",
    "import jax.experimental.multihost_utils\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    level=logging.INFO,\n",
    ")\n",
    "import argparse\n",
    "import glob\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import flax.nnx as nnx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "import orbax.checkpoint as ocp\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.lr_scheduler import LEARNING_RATE_SCHEDULES, get_learning_rate_scheduler\n",
    "from src.model import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(10000, 4, 512, 2048, 4, rngs=nnx.Rngs(params=0, dropout=1), context_length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_devices = jax.local_device_count()\n",
    "mesh = jax.sharding.Mesh(jax.experimental.mesh_utils.create_device_mesh((num_devices,)), (\"data\",))\n",
    "model_sharding = jax.NamedSharding(mesh, jax.sharding.PartitionSpec())\n",
    "data_sharding = jax.NamedSharding(mesh, jax.sharding.PartitionSpec(\"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_fn = get_learning_rate_scheduler(\n",
    "    \"linear_warmup_cosine_decay\",\n",
    "    lr=0.01,\n",
    "    warmup_steps=400,\n",
    "    decay_steps=400,\n",
    "    total_steps=10000,\n",
    ")\n",
    "optimizer = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),\n",
    "    optax.adamw(\n",
    "        learning_rate=learning_rate_fn,\n",
    "        b1=0.9,\n",
    "        b2=0.95,\n",
    "        weight_decay=0.1,\n",
    "    ),\n",
    ")\n",
    "state = nnx.state((model, optimizer))\n",
    "state = jax.device_put(state, model_sharding)\n",
    "nnx.update((model, optimizer), state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = nnx.Optimizer(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, x, y):\n",
    "    logits = model(x)\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits.reshape(-1, 10000), y.reshape(-1))\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "def train_step(model, x, y):\n",
    "    grad_fn = nnx.value_and_grad(loss_fn)\n",
    "    loss, grad = grad_fn(model, x, y)\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randint(0, 10000, (4, 1024))\n",
    "y = x\n",
    "\n",
    "x, y = jax.device_put((x, y), data_sharding)\n",
    "jax.debug.visualize_array_sharding(x)\n",
    "loss, grad = train_step(state.model, x, y)\n",
    "jax.debug.visualize_array_sharding(grad[\"token_emb\"][\"embedding\"].value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
