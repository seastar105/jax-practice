{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b3a556-f365-4d78-8437-4cf76ab55289",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U torch 'jax[tpu]' ipykernel jupyter optax flax 'numpy<2.0' 'datasets[audio]' transformers orbax matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c4ecad-e262-4998-ad7c-7a5b354f8081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "\n",
    "set_matplotlib_formats(\"svg\", \"pdf\")  # For export\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import to_rgba\n",
    "\n",
    "sns.set()\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f255f3e-4637-4d41-91a7-bdc25992e1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "import flax.nnx as nnx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "print(\"Using jax\", jax.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1738c5c7-77da-4704-9a0b-e2db932b2f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(nnx.Module):\n",
    "    def __init__(self, dim_in: int, dim_out: int, rngs):\n",
    "        self.fc1 = nnx.Linear(dim_in, dim_out, rngs=rngs)\n",
    "        self.fc2 = nnx.Linear(dim_out, 1, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.fc2(nnx.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5fe740-e611-4754-816d-6c703605f923",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "rng, inp_rng = jax.random.split(rng)\n",
    "inp = jax.random.normal(inp_rng, (8, 2))  # Batch size 8, input size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16226190-92e2-44c9-b9d0-bada582c0d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "\n",
    "class XORDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, size, seed, std=0.1):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            size - Number of data points we want to generate\n",
    "            seed - The seed to use to create the PRNG state with which we want to generate the data points\n",
    "            std - Standard deviation of the noise (see generate_continuous_xor function)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.np_rng = np.random.RandomState(seed=seed)\n",
    "        self.std = std\n",
    "        self.generate_continuous_xor()\n",
    "\n",
    "    def generate_continuous_xor(self):\n",
    "        # Each data point in the XOR dataset has two variables, x and y, that can be either 0 or 1\n",
    "        # The label is their XOR combination, i.e. 1 if only x or only y is 1 while the other is 0.\n",
    "        # If x=y, the label is 0.\n",
    "        data = self.np_rng.randint(low=0, high=2, size=(self.size, 2)).astype(np.float32)\n",
    "        label = (data.sum(axis=1) == 1).astype(np.int32)\n",
    "        # To make it slightly more challenging, we add a bit of gaussian noise to the data points.\n",
    "        data += self.np_rng.normal(loc=0.0, scale=self.std, size=data.shape)\n",
    "\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of data point we have. Alternatively self.data.shape[0], or self.label.shape[0]\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the idx-th data point of the dataset\n",
    "        # If we have multiple things to return (data point and label), we can return them as tuple\n",
    "        data_point = self.data[idx]\n",
    "        data_label = self.label[idx]\n",
    "        return data_point, data_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cd4175-a787-470d-b657-be2fa3d98b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = XORDataset(size=200, seed=42)\n",
    "print(\"Size of dataset:\", len(dataset))\n",
    "print(\"Data point 0:\", dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb020a9-706b-416b-b056-4e0f5a8fe5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_samples(data, label):\n",
    "    data_0 = data[label == 0]\n",
    "    data_1 = data[label == 1]\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.scatter(data_0[:, 0], data_0[:, 1], edgecolor=\"#333\", label=\"Class 0\")\n",
    "    plt.scatter(data_1[:, 0], data_1[:, 1], edgecolor=\"#333\", label=\"Class 1\")\n",
    "    plt.title(\"Dataset samples\")\n",
    "    plt.ylabel(r\"$x_2$\")\n",
    "    plt.xlabel(r\"$x_1$\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851ae41d-3faa-4f54-8734-74adfae6f17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_samples(dataset.data, dataset.label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2752da78-4c3c-4b78-9a04-ab13bf492292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple, list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "\n",
    "data_loader = data.DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=numpy_collate, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d306f52-5725-4f60-b708-7f61a7afbff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(...)) catches the first batch of the data loader\n",
    "# If shuffle is True, this will return a different batch every time we run this cell\n",
    "# For iterating over the whole dataset, we can simple use \"for batch in data_loader: ...\"\n",
    "data_inputs, data_labels = next(iter(data_loader))\n",
    "\n",
    "# The shape of the outputs are [batch_size, d_1,...,d_N] where d_1,...,d_N are the\n",
    "# dimensions of the data point returned from the dataset class\n",
    "print(\"Data inputs\", data_inputs.shape, \"\\n\", data_inputs)\n",
    "print(\"Data labels\", data_labels.shape, \"\\n\", data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a89ab3f-cb97-456e-ba1f-f1a4a4156032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "model = MyModule(2, 10, nnx.Rngs(0))\n",
    "optimizer = nnx.Optimizer(model, optax.adamw(learning_rate=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b641bf-2f61-4e1d-a88b-7191fb7ccd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, batch):\n",
    "    data_inputs, labels = batch\n",
    "    # Obtain the logits and predictions of the model for the input data\n",
    "    logits = model(data_inputs).squeeze(-1)\n",
    "    pred_labels = (logits > 0).astype(jnp.float32)\n",
    "    # Calculate the loss and accuracy\n",
    "    loss = optax.sigmoid_binary_cross_entropy(logits, labels).mean()\n",
    "    acc = (pred_labels == labels).mean()\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "batch = next(iter(data_loader))\n",
    "loss_fn(model, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df33c745-c7dc-46a9-8b1f-d10a0e9be2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.jit\n",
    "def train_step(model, optimizer, batch):\n",
    "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, acc), grads = grad_fn(model, batch)\n",
    "    optimizer.update(grads)\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2daa24-1635-4e32-86f8-b18460bf664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.jit  # Jit the function for efficiency\n",
    "def eval_step(model, batch):\n",
    "    # Determine the accuracy\n",
    "    _, acc = loss_fn(model, batch)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c43df9-f7b7-4f0d-9b55-8856af2161f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = XORDataset(size=2500, seed=42)\n",
    "train_data_loader = data.DataLoader(\n",
    "    train_dataset, batch_size=128, shuffle=True, collate_fn=numpy_collate, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb0cf2e-7e03-48f7-b849-fd3779bc06cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        loss, acc = train_step(model, optimizer, batch)\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(loss, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7776b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
